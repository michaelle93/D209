--Decision Tree for classification--

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
			stratify=y, random_state=1)
dt= DecisionTreeClassifier(max_depth=2, random_state=1)

# fit dt to the training set
dt.fit(X_train, y_train)

# predict the test set labels
y_pred = dt.predict(X_test)

# evaluate the test set accuracy
accuracy_score(y_test, y_pred)

--voting classifier in sklearn--

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier

SEED=1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,
						random_state= SEED)
lr = LogisticRegression(random_state=SEED)
knn = KNN(n_neighbors=??)
dt = DecisionTreeClassifier(random_state=SEED)
classifiers = [('Logistic Regression', lr),
		('K Nearest Neighboours',  knn),
		('Classification Tree', dt)]

for clf_name, clf in classifiers:
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))

vc = VotingClassifier(estimators=classifiers)

vc.fit(X_train, y_train)
y_pred = vc.predict(X_test)

print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))



--Bagging---

	bootstrap Aggregation
	bagging classification/regression
		classification:
			aggregates predictions by majority voting
			BaggingClassifier in scikit-learn
		Regression:
			aggregates predictions through averaging
			BaggingRegressor in scikit-learn

--Bagging classifier in sklearn--

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

SEED=1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							stratify=y,
							random_state=SEED)
dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)
bc= BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)
bc.fit(X_train, y_train)
y_pred = bc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))


--out of bag evaluation--

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
SEED=1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							stratify=y,
							random_state=SEED)
dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)

bc= BaggingClassifier(base_estimator=dt, n_estimators=300, 
			oob_score=True, n_jobs=-1)
bc.fit(X_train, y_train)
y_pred = bc.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
oob_accuracy = bc.oob_score_
print('Test set accuracy: {:.3f}'.format(test_accuracy))
print('OOB accuracy: {:.3f}'.format(oob_accuracy))


--Random Forests--


from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE
SEED=1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							stratify=y,
							random_state=SEED)
rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

rmse_test = MSE(y_test, y_pred)**(1/2)
print('Test set RMSE of rf: {:.2f}'.format(rmse_test))

	-feature importance-

import pandas as pd
import matplotlib.pyplot as plt
importances_rf = pd.Series(rf.feature_importances_, index = X.columns)
sorted_importances_rf = importances_rf.sort_values()
sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()


--AdaBoost--
	
	Boosting: ensembel method of combining several weak learners to form a strong learner
	Weak learner: model doing slightly better than random guessing
	
Adaboost - adaptive boosting


from skelarn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

SEED = 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							stratify=y,
							random_state=SEED)
dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)

adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)

adb_clf.fit(X_train, y_train)

y_pred_proba = adb_clf.predict_proba(X_test)[:,1]

adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)

print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))


--Gradient Boosted Trees--

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE

SEED=1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							random_state=SEED)

gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)

gbt.fit(X_train, y_train)

y_pred = gbt.predict(X_test)

rmse_test = MSE(y_test, y_pred)**(1/2)

print('Test set RMSE: {:.2f}'.format(rmse_test))



--Stochastic Gradient Boosting--


from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE

SEED=1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
							random_state=SEED)

sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2,
					n_estimators=300, random_state=SEED)

sgbt.fit(X_train, y_train)

y_pred = sgbt.predict(X_test)

rmse_test = MSE(y_test, y_pred)**(1/2)

print('Test set RMSE: {:.2f}'.format(rmse_test))


--Hyperparameters--

	-inspecting the hyperparameters of a CART in sklearn-

from sklearn.tree import DecisionTreeClassifier

SEED=1

dt = DecisionTreeClassifier(random_state=SEED)

print(dt.get_params())


from sklearn.model_selection import GridSearchCV

params_dt = { 'max_depth':[3,4,5,6],
		'min_samples_leaf': [0.04, 0.06, 0.08],
		'max_features': [0.2, 0.4, 0.6, 0.8]
		}

grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='accuracy', cv=10,
				n_jobs=-1)
grid_dt.fit(X_train, y_train)

best_hyperparams = grid_dt.best_params_
print('Best hyperparameters:\n', best_hyperparams)

best_CV_score = grid_dt.best_score_
print('Best CV accuracy'.format(best_CV_score))

best_model = grid_dt.best_estimator_

test_acc = best_model.score(X_test, y_test)

print('Test set accuracy of best model: {:.3f}'.format(test_acc))















































































































































































































